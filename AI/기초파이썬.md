##### DACON 사이트에 올라온 기초 파이썬 부분을 공부하였다.
##### [ Lv3. 교차검증과 LGBM 모델을 활용한 와인 품질 분류하기 ] 부분의 내용을 정리하였다.
##### [DACON링크](https://dacon.io/competitions/open/235698/overview/description)


단계1. EDA

데이터를 분석하기 전에 그래프 등을 이용하여 데이터를 관찰하는 과정

- 파일 가져오기 : read_csv
- 피쳐, 칼럼명 확인 : df.shape, df.head()
- 결측치 확인하기 : pd.isnull().sum()
- 데이터의 통계량(count, mean, std, min, max, 25%, 50%, 75%) 요약 : df.dscribe()
- 시각화
  - matplotlib, seaborn 라이브러리 이용
  - 머신러닝의 방향성을 잡기에 좋다.
  - copy()로 복사본 생성 후 진행


단계2. 전처리

정교한 예측 분석을 위해 데이터를 가공하는 과정
결측치, 이상치 등을 확인 및 제거하고 데이터를 일관성 있게 만든다.
종류 : 데이터 클리닝, 데이터 통합, 데이터 변환, 데이터 축소, 데이터 이산화 등

- 이상치탐지 : IQR(Inter Qunatile Range) - boxplot이용해서 확인 가능

- 이상치 제거
  
  (1) IQR찾기
  1. 25%에 위치한 값 찾기 : np.quantile(df['column'],0.25)
  2. 75%에 위치한 값 찾기 : np.quantile(df['column'],0.75)
  3. IQR 계산 : 75%에 위치한 값 - 25%에 위치한 값
  
  (2) 이상치 제거
  1. 25%에 위치한 값보다 1.5*IQR 작은 값 찾기 : 25%에 위치한 값 - 1.5*IQR
  2. 75%에 위치한 값보다 1.5*IQR 큰 값 찾기 : 75%에 위치한 값 + 1.5*IQR
  3. 1에서 찾은 값 이상, 2에서 찾은 값 이하인 것만 뽑아내기

- 수치형 데이터 정규화
  평활 함수 모델은 숫자 크기와 단위에 영향을 많이 받는다.
  Min Max Scailing
    - 가장 작은 값 0으로, 가장 큰 값 1로 만든다.
    - 그 사이 값들은 0~1사이에 분포
    - 이상치에 민감하다.

- One-Hot Encoding
  문자로된 데이터는 학습을 위해 인코딩 해야한다.
  하나만 Hot, 나머지는 Cold
  자신에게 맞는 것은 1, 나머지는 0으로 바꾼다.


단계3. 모델링

- RandomForest : 여러개의 나무를 만들어서 그들을 취합하여 결론을 내린다.
                 ( from sklearn.ensemble import RandomForestClassifier )

- 학습시키기
  - RandomForestClassifier() 분류 모형 저장
  - 종속변수 제거 데이터 프레임 생성
  - 종속변수 열 생성
  - RandomForestClassifier()에 종속변수 열과 종속변수 제거 데이터 프레임 학습시키기

- 교차검증
  - Hold-out : 예측 성능 가늠을 위해 Train데이터를 train, valid로 나누는 것 보통 8:2, 7:3으로 나눈다.
             이 방법은 데이터가 낭비된다. -> 교차검증 이용!
  - K-Fold : 모든 데이터를 최소한 한 번씩 학습시키기


단계4. 튜닝
 - Hyperparameter Optimization 
  - 하이퍼파라미터(학습에서 사전에 설정하는 값)의 최적값 탐색
  - Manual Search, Grid Search, Random Search, Bayesian optimization 등의 방법이 있다.
  
 - Grid Search : 탐색할 값들을 미리 지정하고, 그들의 모든 조합을 바탕으로 성능의 최고점을 찾는다.
               : 원하는 범위를 정확하게 비교분석 가능, 시간이 오래걸림, 성능의 최고점이 아닐 가능성 높음
 
 - Random Search : 탐색할 값들의 범위를 미리 지정하고 그 범위에서 가능한 조합을 바탕으로 최고점을 찾는다.
                 : 시간이 짧게 걸림, 성능이 좋은 경우도 생기고 나쁜 경우도 생김
 
 - Bayesian Optimization : 하이퍼파라미터 범위 지정 후 무작위로 R번 탐색 후 B번 만큼 최적의 값을 찾는다.
                         : 시간이 짧게 걸림, 신뢰성 있음, 최적의 값 찾기 가능
                         : Random하게 찍은 값이 달라지면, 최적화 과정이 오래 걸릴 수 있다.
                         : Random하게 찍은 값이 부족하면, 최적의 값 탐색 불가능할 수 있음
                         : Random하게 찍은 값이 너무 많으면, 최적화 이전에 이미 최적값을 가질 수 있음



[ 마지막에 수행되는 과정 ]
1. K-Fold로 train과 valid 나누기
2. Model로 train데이터 학습
3. Model로 valid 데이터 예측 및 성능 확인
4. Model로 test데이터 예측
5. 예측 결과에 대한 최빈값을 이용해서 결정
 
